#!/usr/bin/python

import os
import urllib2
import lxml.etree
import lxml.html
import urlparse

# This respects http_proxy
def get_from_url(url):
    #doc = lxml.etree.parse(url) # doesn't use proxy
    #return lxml.etree.parse(urllib2.urlopen(url))
    s = urllib2.urlopen(url).read()
    #print s
    return lxml.etree.fromstring(s)

def get_html_from_url(url):
    #doc = lxml.etree.parse(url) # doesn't use proxy
    #return lxml.etree.parse(urllib2.urlopen(url))
    s = urllib2.urlopen(url).read()
    #print s
    return lxml.html.fromstring(s)

def get_place(li):
    for sp in li.xpath("span[@class=\"place\"]/text()"):
        return sp
    return None

def get_li(doc, place):
    for li in doc.xpath("li"):
        if get_place(li) == place:
            return li
    return None    

def get_ahref(li, alt):
    for a in li.xpath("a[@class=\"nav\"]"):
        for imgalt in a.xpath("img/@alt"):
            if imgalt == alt:
                return a.attrib["href"]
    return None

def get_next(url, code):
    doc = get_from_url(
        "http://lesswrong.com/api/article_navigation?article_id={0}".format(code))
    #print lxml.etree.tostring(doc)
    li = get_li(doc, "by author")
    if li is None: return None
    ahref = get_ahref(li, "Next")
    if ahref is None: return None
    return urlparse.urljoin(url, ahref)

def get_title(content):
    for t in content.xpath("//h1/a/text()"):
        return t

def get_body(content, code):
    for frag in content.xpath("//div[@id=\"entry_t3_{0}\"]/div/div".format(code)):
        return frag
    return None

def get_date(content):
    for frag in content.xpath("//span[@class=\"date\"]/text()"):
        return frag
    return None

def is_dodgy(url):
    parsed = urlparse.urlparse(url)
    if parsed.hostname and parsed.hostname.endswith("lesswrong.com"):
        return parsed.hostname != "wiki.lesswrong.com"
    if not parsed.hostname and not parsed.path.startswith("/lw/"):
        return True
    return False

def goodpath(path):
    assert len(path) == 5
    assert path[0] == ""
    assert path[1] == "lw"
    assert path[4] == ""

pairs = []

url = "http://lesswrong.com/lw/gn/the_martial_art_of_rationality/"
for i in range(100):
    print "    ", url
    path = urlparse.urlparse(url).path.split("/")
    goodpath(path)
    filename = "/".join(path[1:-1]) + ".html"
    os.makedirs("/".join(path[1:-2]))
    code = path[2]
    content = get_html_from_url(url)
    #with open("/".join(path[1:-1]) + ".raw.html", "w") as f:
    #    f.write(lxml.html.tostring(content, pretty_print=True))
    title = get_title(content)
    print "     -----", title
    pairs.append((title, filename))
    date = get_date(content)
    print "     -----", date
    entry = get_body(content, code)
    d = lxml.etree.Element("p")
    d.text = date
    entry.insert(0, d)
    h1 = lxml.etree.Element("h1")
    h1.text = title
    entry.insert(0, h1)
    #lxml.etree.SubElement(entry, "hr")
    
    #print lxml.html.tostring(entry, pretty_print=True)
    for a in entry.xpath(".//a"):
        if not a.attrib.has_key("href"):
            continue
        href = a.attrib["href"]
        parsed = urlparse.urlparse(href)
        if not parsed.hostname:
            hpath = parsed.path.split("/")
            goodpath(path)
            a.attrib["href"] = "../{0}/{1}.html".format(hpath[2], hpath[3])
        #if is_dodgy(href):
        #    print "href:", href
    #for src in entry.xpath(".//img/@src"):
    #    if is_dodgy(src):
    #        print "src:",src
    with open(filename, "w") as f:
        f.write(lxml.html.tostring(entry))
    #print
    #print "===================================================="
    #print
    url = get_next(url, code)
    if url is None:
        break

with open("index.html", "w") as f:
    f.write("<html>")
    f.write("<head><title>Eliezer Yudkowsky</title></head>")
    f.write("<body><ul>")
    for t, h in pairs:
        f.write("<li><a href=\"{0}\">{1}</a></li>".format(h, t))
    f.write("</ul></body></html>")
#get_next("gn")
#url = get_next("go")
#print get_urlcode(url)

