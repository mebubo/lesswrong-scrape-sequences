#!/usr/bin/python

import os
import urllib
import urllib2
import lxml.etree
import lxml.html
from lxml.html import builder as E
import urlparse

# This respects http_proxy
def get_from_url(url):
    #print url
    #doc = lxml.etree.parse(url) # doesn't use proxy
    #return lxml.etree.parse(urllib2.urlopen(url))
    s = urllib2.urlopen(url).read()
    #print s
    return lxml.etree.fromstring(s)

def get_html_from_url(url):
    #doc = lxml.etree.parse(url) # doesn't use proxy
    #return lxml.etree.parse(urllib2.urlopen(url))
    s = urllib2.urlopen(url).read()
    #print s
    return lxml.html.fromstring(s)

def get_place(li):
    for sp in li.xpath("span[@class=\"place\"]/text()"):
        return sp
    return None

def get_li(doc, place):
    for li in doc.xpath("li"):
        if get_place(li) == place:
            return li
    return None    

def get_ahref(li, alt):
    for a in li.xpath("a[@class=\"nav\"]"):
        for imgalt in a.xpath("img/@alt"):
            if imgalt == alt:
                return unicode(a.attrib["href"])
    return None

def get_next(url, code):
    if code == "t6":
        return "http://lesswrong.com/lw/t7/dumb_deplaning/"
    doc = get_from_url(
        "http://lesswrong.com/api/article_navigation?article_id={0}".format(code))
    #print lxml.etree.tostring(doc)
    li = get_li(doc, "by author")
    if li is None: return None
    ahref = get_ahref(li, "Next")
    if ahref is None: return None
    print ahref
    return urlparse.urljoin(url, urllib.quote(ahref.encode("utf-8")))

def get_title(content):
    for t in content.xpath("//h1/a/text()"):
        return unicode(t)

def get_body(content, code):
    for frag in content.xpath("//div[@id=\"entry_t3_{0}\"]/div/div".format(code)):
        return frag
    return None

def get_date(content):
    for frag in content.xpath("//span[@class=\"date\"]/text()"):
        return unicode(frag)
    return None

def is_dodgy(url):
    parsed = urlparse.urlparse(url)
    if parsed.hostname and parsed.hostname.endswith("lesswrong.com"):
        return parsed.hostname != "wiki.lesswrong.com"
    if not parsed.hostname and not parsed.path.startswith("/lw/"):
        return True
    return False

def goodpath(path):
    return (len(path) == 5
        and path[0] == ""
        and path[1] == "lw"
        and path[4] == "")

pairs = []

url = "http://lesswrong.com/lw/gn/the_martial_art_of_rationality/"
while True:
    print "    ", url
    path = urlparse.urlparse(url).path.split("/")
    assert(goodpath(path))
    filename = "/".join(path[1:-1]) + ".html"
    os.makedirs("/".join(path[1:-2]))
    code = path[2]
    content = get_html_from_url(url)
    #with open("/".join(path[1:-1]) + ".raw.html", "w") as f:
    #    f.write(lxml.html.tostring(content, pretty_print=True))
    title = get_title(content)
    print "     -----", title
    pairs.append((title, filename))
    date = get_date(content)
    print "     -----", date
    entry = get_body(content, code)
    for a in entry.xpath(".//a"):
        if not a.attrib.has_key("href"):
            continue
        href = a.attrib["href"]
        parsed = urlparse.urlparse(href)
        if not parsed.hostname:
            hpath = parsed.path.split("/")
            if goodpath(hpath):
                a.attrib["href"] = u"../{0}/{1}.html".format(hpath[2], hpath[3])
            else:
                a.attrib["href"] = urlparse.urljoin(url, href)

    page = E.HTML(
        E.HEAD(
            E.TITLE(title)
        ), E.BODY(
            E.H1(title),
            E.P(date),
            *list(entry)
        )
    )

    with open(filename, "w") as f:
        f.write(lxml.html.tostring(page))
    url = get_next(url, code)
    if url is None:
        break

page = E.HTML(
    E.HEAD(
        E.TITLE("Sequences"),
    ), E.BODY(
        E.UL(
            *[
                E.LI(E.A(t, href=h))
                for t, h in pairs
             ]
        )
    )
)
with open("index.html", "w") as f:
        f.write(lxml.html.tostring(page))
    

