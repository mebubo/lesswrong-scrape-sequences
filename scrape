#!/usr/bin/python

import itertools
import os
import base64
import httplib
import urllib
import urllib2
import urlparse
import lxml.etree
import lxml.html
from lxml.html import builder as E

def makedirs(t):
    if not os.path.isdir(t):
        os.makedirs(t)

# Very very simple memoize suitable only for these exact circumstances
def memoize(dr):
    def memf(func):
        def memoized(url):
            centry = "cache/{0}/{1}".format(
                dr, base64.b64encode(url, "_-"))
            if os.path.isfile(centry):
                with open(centry) as f:
                    return f.read()
            res = func(url)
            with open(centry, "w") as f:
                f.write(res)
            return res
        return memoized
    return memf
            
# This respects http_proxy
@memoize("urlcontent")
def read_url(url):
    print "Getting: ", url
    return urllib2.urlopen(url).read()

conmap = {}

@memoize("301")
def check_for_301(url):
    print "Checking for 301: ", url
    purl = urlparse.urlparse(url)
    if purl.hostname not in conmap:
        conmap[purl.hostname] = httplib.HTTPConnection(purl.hostname)
    conn = conmap[purl.hostname]
    conn.request("HEAD", url)
    resp = conn.getresponse()
    try:
        if resp.status == 301:
            return resp.getheader('location')
        else:
            return url
    finally:
        resp.read()

def get_from_url(url):
    return lxml.etree.fromstring(read_url(url))

def get_html_from_url(url):
    return lxml.html.fromstring(read_url(url))

def get_place(li):
    for sp in li.xpath("span[@class=\"place\"]/text()"):
        return sp
    return None

def get_li(doc, place):
    for li in doc.xpath("li"):
        if get_place(li) == place:
            return li
    return None    

def get_ahref(li, alt):
    for a in li.xpath("a[@class=\"nav\"]"):
        for imgalt in a.xpath("img/@alt"):
            if imgalt == alt:
                return unicode(a.attrib["href"])
    return None
    
def goodpath(path):
    return (len(path) == 5
        and path[0] == ""
        and path[1] == "lw"
        and path[4] == "")

def get_title(content):
    for t in content.xpath("//h1/a/text()"):
        return unicode(t)

def get_body(content, code):
    for frag in content.xpath("//div[@id=\"entry_t3_{0}\"]/div/div".format(code)):
        return frag
    return None

def get_date(content):
    for frag in content.xpath("//span[@class=\"date\"]/text()"):
        return unicode(frag)
    return None

class Blogpost(object):
    def __init__(self, sequence, url):
        self.sequence = sequence
        self.url = url
        path = urlparse.urlparse(url).path.split("/")
        assert(goodpath(path))
        self.code = path[2]
        #self.filename = "/".join(path[1:-1]) + ".html"
        #self.dir = "/".join(path[1:-2])
        self.filename = "{0:04d}.html".format(sequence)
    
    def get_nexturl(self):
        if self.code == "t6":
            return "http://lesswrong.com/lw/t7/dumb_deplaning/"
        doc = get_from_url(
            "http://lesswrong.com/api/article_navigation?article_id={0}".format(self.code))
        li = get_li(doc, "by author")
        if li is None: return None
        ahref = get_ahref(li, "Next")
        if ahref is None: return None
        return urlparse.urljoin(self.url, 
            urllib.quote(ahref.encode("utf-8")))

    def write(self, urlmap):
        #os.makedirs("target/" + self.dir)
        content = get_html_from_url(self.url)
        self.title = get_title(content)
        print "     -----", self.title
        self.date = get_date(content)
        print "     -----", self.date
        entry = get_body(content, self.code)
        for a in entry.xpath(".//a"):
            if not a.attrib.has_key("href"):
                continue
            href = urlparse.urljoin(url, a.attrib["href"])
            if href.startswith("http://www.overcomingbias.com/"):
                href = check_for_301(href)
            if href in urlmap:
                print "URL Mapping {0} to {1}".format(
                    href,urlmap[href].filename)
                a.attrib["href"] = urlmap[href].filename
            else:
                print "URL Preserving {0} which is {1} + {2}".format(
                    href, url, a.attrib["href"])
                a.attrib["href"] = href

        page = E.HTML(
            E.HEAD(
                E.TITLE(self.title)
            ), E.BODY(*([
                E.H1(self.title),
                E.P("Eliezer Yudkowsky, " + self.date),
                ] + list(entry) + [
                E.P(E.I("Original with comments: ", 
                    E.A(self.title, href=self.url)))
            ]))
        )

        with open("target/" + self.filename, "w") as f:
            f.write(lxml.html.tostring(page))

def scrape_all(start_url, sequencer):
    print "Collecting URLs"
    posts = []
    url = start_url
    for seq in sequencer:
        print "...", url
        post = Blogpost(seq, url)
        posts.append(post)
        url = post.get_nexturl()
        if url is None:
            break

    urlmap = dict((p.url, p) for p in posts)

    print "Writing posts"
    for i, post in enumerate(posts):
        print "URL: ", post.url, "{0}/{1}".format(i+1, len(posts))
        post.write(urlmap)

    page = E.HTML(
        E.HEAD(
            E.TITLE("Sequences"),
        ), E.BODY(
            E.UL(
                *[
                    E.LI(E.A(p.title, href=p.filename))
                    for p in posts
                 ]
            )
        )
    )
    with open("target/index.html", "w") as f:
            f.write(lxml.html.tostring(page))
    
os.mkdir("target")
makedirs("cache/urlcontent")
makedirs("cache/301")

url = "http://lesswrong.com/lw/gn/the_martial_art_of_rationality/"
#for seq in itertools.count(1): # ie forever
scrape_all(url, range(1,101))

